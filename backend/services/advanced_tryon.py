import cv2
import numpy as np
from PIL import Image
import requests
import io
import base64
import json

class AdvancedTryOnService:
    def __init__(self):
        # Free AI models for virtual try-on
        self.models = {
            'cloth_segmentation': 'https://api-inference.huggingface.co/models/mattmdjaga/segformer_b2_clothes',
            'pose_estimation': 'https://api-inference.huggingface.co/models/microsoft/table-transformer-structure-recognition',
            'inpainting': 'https://api-inference.huggingface.co/models/runwayml/stable-diffusion-inpainting'
        }
        
    async def advanced_virtual_tryon(self, person_bytes, garment_bytes, product_info=None):
        try:
            print(f"ðŸš€ Starting advanced virtual try-on...")
            
            # Use improved pose-based overlay (most reliable)
            result = await self._basic_overlay(person_bytes, garment_bytes)
            
            print(f"âœ… Virtual try-on completed successfully")
            return result
            
        except Exception as e:
            print(f"âŒ Advanced try-on failed: {e}")
            # Ultimate fallback
            return await self._simple_fallback(person_bytes, garment_bytes)
    
    async def _segment_clothing(self, image_bytes):
        """Segment clothing areas using Hugging Face model"""
        try:
            # Convert to base64
            image_b64 = base64.b64encode(image_bytes).decode()
            
            response = requests.post(
                self.models['cloth_segmentation'],
                headers={"Authorization": "Bearer hf_demo"},  # Use demo token
                json={"inputs": image_b64},
                timeout=30
            )
            
            if response.status_code == 200:
                # Process segmentation result
                result = response.json()
                # Convert back to mask
                return self._process_segmentation_result(result)
            
        except Exception as e:
            print(f"Segmentation failed: {e}")
            
        # Fallback: create simple mask
        return self._create_simple_mask(image_bytes)
    
    async def _extract_garment_features(self, garment_bytes):
        """Extract garment features for better fitting"""
        image = Image.open(io.BytesIO(garment_bytes))
        image_np = np.array(image)
        
        # Extract color palette
        colors = self._extract_dominant_colors(image_np)
        
        # Detect garment type
        garment_type = self._classify_garment_type(image_np)
        
        # Extract texture features
        texture = self._analyze_texture(image_np)
        
        return {
            'colors': colors,
            'type': garment_type,
            'texture': texture,
            'size': image.size
        }
    
    async def _generate_tryon(self, person_bytes, garment_bytes, mask):
        """Generate realistic try-on using AI inpainting"""
        try:
            person_img = Image.open(io.BytesIO(person_bytes))
            garment_img = Image.open(io.BytesIO(garment_bytes))
            
            # Resize garment to fit person
            fitted_garment = self._fit_garment_to_body(person_img, garment_img, mask)
            
            # Blend with proper lighting and shadows
            result = self._advanced_blend(person_img, fitted_garment, mask)
            
            return result
            
        except Exception as e:
            print(f"Generation failed: {e}")
            return await self._basic_overlay(person_bytes, garment_bytes)
    
    def _fit_garment_to_body(self, person_img, garment_img, mask):
        """Fit garment to body shape using perspective transformation"""
        person_np = np.array(person_img)
        garment_np = np.array(garment_img)
        
        # Find body contours from mask
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        if contours:
            # Get bounding rectangle of largest contour
            largest_contour = max(contours, key=cv2.contourArea)
            x, y, w, h = cv2.boundingRect(largest_contour)
            
            # Resize garment to fit
            garment_resized = cv2.resize(garment_np, (w, h))
            
            # Apply perspective transformation for realistic fit
            garment_fitted = self._apply_body_curvature(garment_resized, largest_contour)
            
            return garment_fitted
        
        return garment_np
    
    def _advanced_blend(self, person_img, garment_img, mask):
        """Advanced blending with lighting and shadow effects"""
        person_np = np.array(person_img)
        garment_np = np.array(garment_img)
        
        # Create seamless blend
        result = person_np.copy()
        
        # Apply garment with proper alpha blending
        if garment_np.shape[:2] == mask.shape:
            # Normalize mask
            mask_norm = mask.astype(float) / 255.0
            mask_3d = np.stack([mask_norm] * 3, axis=-1)
            
            # Blend with lighting adjustment
            lighting_factor = self._calculate_lighting(person_np, mask)
            garment_lit = garment_np * lighting_factor
            
            # Seamless blending
            blended = person_np * (1 - mask_3d) + garment_lit * mask_3d
            result = blended.astype(np.uint8)
        
        return Image.fromarray(result)
    
    def _calculate_lighting(self, person_img, mask):
        """Calculate lighting conditions from person image"""
        # Analyze lighting in non-clothing areas
        inverse_mask = 255 - mask
        lighting_areas = cv2.bitwise_and(person_img, person_img, mask=inverse_mask)
        
        # Calculate average brightness
        brightness = np.mean(lighting_areas[lighting_areas > 0])
        
        # Normalize to 0.7-1.3 range
        lighting_factor = max(0.7, min(1.3, brightness / 128.0))
        
        return lighting_factor
    
    def _apply_body_curvature(self, garment, contour):
        """Apply body curvature to garment for realistic draping"""
        # Simplified curvature application
        h, w = garment.shape[:2]
        
        # Create slight curvature effect
        map_x = np.zeros((h, w), dtype=np.float32)
        map_y = np.zeros((h, w), dtype=np.float32)
        
        for i in range(h):
            for j in range(w):
                # Apply subtle barrel distortion for body curvature
                center_x, center_y = w // 2, h // 2
                dx = j - center_x
                dy = i - center_y
                
                r = np.sqrt(dx*dx + dy*dy)
                if r > 0:
                    # Subtle curvature factor
                    factor = 1 + 0.0001 * r
                    map_x[i, j] = center_x + dx * factor
                    map_y[i, j] = center_y + dy * factor
                else:
                    map_x[i, j] = j
                    map_y[i, j] = i
        
        # Apply transformation
        curved_garment = cv2.remap(garment, map_x, map_y, cv2.INTER_LINEAR)
        return curved_garment
    
    def _extract_dominant_colors(self, image):
        """Extract dominant colors from garment"""
        from sklearn.cluster import KMeans
        
        pixels = image.reshape(-1, 3)
        kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
        kmeans.fit(pixels)
        
        colors = kmeans.cluster_centers_.astype(int)
        return [f"#{r:02x}{g:02x}{b:02x}" for r, g, b in colors]
    
    def _classify_garment_type(self, image):
        """Classify garment type based on shape analysis"""
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        contours, _ = cv2.findContours(gray > 50, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        if contours:
            largest = max(contours, key=cv2.contourArea)
            x, y, w, h = cv2.boundingRect(largest)
            
            aspect_ratio = w / h
            
            if aspect_ratio > 1.2:
                return 'shirt'
            elif aspect_ratio < 0.6:
                return 'dress'
            else:
                return 'top'
        
        return 'unknown'
    
    def _analyze_texture(self, image):
        """Analyze garment texture"""
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        
        # Calculate texture features
        variance = np.var(gray)
        
        if variance > 1000:
            return 'textured'
        elif variance > 500:
            return 'medium'
        else:
            return 'smooth'
    
    def _process_segmentation_result(self, result):
        """Process Hugging Face segmentation result"""
        # This would process the actual API response
        # For now, return a simple mask
        return np.ones((512, 512), dtype=np.uint8) * 255
    
    def _create_simple_mask(self, image_bytes):
        """Create simple clothing mask as fallback"""
        image = Image.open(io.BytesIO(image_bytes))
        image_np = np.array(image)
        
        # Simple torso region mask
        h, w = image_np.shape[:2]
        mask = np.zeros((h, w), dtype=np.uint8)
        
        # Create torso region
        cv2.rectangle(mask, (w//4, h//4), (3*w//4, 3*h//4), 255, -1)
        
        return mask
    
    async def _basic_overlay(self, person_bytes, garment_bytes):
        """Improved overlay with pose detection"""
        import mediapipe as mp
        
        person_img = Image.open(io.BytesIO(person_bytes))
        garment_img = Image.open(io.BytesIO(garment_bytes))
        
        person_np = np.array(person_img)
        garment_np = np.array(garment_img)
        
        # Use MediaPipe for pose detection
        mp_pose = mp.solutions.pose
        pose = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5)
        
        # Convert BGR to RGB for MediaPipe
        rgb_image = cv2.cvtColor(person_np, cv2.COLOR_RGB2BGR)
        results = pose.process(rgb_image)
        
        h, w = person_np.shape[:2]
        result = person_np.copy()
        
        if results.pose_landmarks:
            # Get key body points
            landmarks = results.pose_landmarks.landmark
            
            # Shoulder points
            left_shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER]
            right_shoulder = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER]
            
            # Hip points
            left_hip = landmarks[mp_pose.PoseLandmark.LEFT_HIP]
            right_hip = landmarks[mp_pose.PoseLandmark.RIGHT_HIP]
            
            # Calculate clothing region based on body landmarks
            shoulder_x1 = int(left_shoulder.x * w)
            shoulder_x2 = int(right_shoulder.x * w)
            shoulder_y = int(min(left_shoulder.y, right_shoulder.y) * h)
            
            hip_x1 = int(left_hip.x * w)
            hip_x2 = int(right_hip.x * w)
            hip_y = int(max(left_hip.y, right_hip.y) * h)
            
            # Expand region for better fit
            clothing_x1 = max(0, min(shoulder_x1, hip_x1) - 30)
            clothing_x2 = min(w, max(shoulder_x2, hip_x2) + 30)
            clothing_y1 = max(0, shoulder_y - 20)
            clothing_y2 = min(h, hip_y + 50)
            
            clothing_w = clothing_x2 - clothing_x1
            clothing_h = clothing_y2 - clothing_y1
            
            if clothing_w > 0 and clothing_h > 0:
                # Resize garment to fit detected body region
                garment_resized = cv2.resize(garment_np, (clothing_w, clothing_h))
                
                # Create smooth mask for blending
                mask = np.ones((clothing_h, clothing_w), dtype=np.float32)
                
                # Apply Gaussian blur for smooth edges
                mask = cv2.GaussianBlur(mask, (21, 21), 0)
                mask = np.stack([mask] * 3, axis=-1)
                
                # Extract ROI from person image
                roi = result[clothing_y1:clothing_y2, clothing_x1:clothing_x2]
                
                # Advanced blending with lighting preservation
                roi_gray = cv2.cvtColor(roi, cv2.COLOR_RGB2GRAY)
                avg_brightness = np.mean(roi_gray) / 255.0
                
                # Adjust garment brightness to match person's lighting
                garment_adjusted = garment_resized * avg_brightness * 1.2
                garment_adjusted = np.clip(garment_adjusted, 0, 255).astype(np.uint8)
                
                # Blend with smooth transition
                blended = roi * (1 - mask * 0.8) + garment_adjusted * (mask * 0.8)
                result[clothing_y1:clothing_y2, clothing_x1:clothing_x2] = blended.astype(np.uint8)
                
                # Add subtle shadow effect
                shadow_mask = mask[:, :, 0] * 0.3
                shadow_offset = 5
                if clothing_y2 + shadow_offset < h and clothing_x2 + shadow_offset < w:
                    shadow_roi = result[clothing_y1+shadow_offset:clothing_y2+shadow_offset, 
                                     clothing_x1+shadow_offset:clothing_x2+shadow_offset]
                    shadow_roi = shadow_roi * (1 - np.stack([shadow_mask] * 3, axis=-1))
                    result[clothing_y1+shadow_offset:clothing_y2+shadow_offset, 
                          clothing_x1+shadow_offset:clothing_x2+shadow_offset] = shadow_roi.astype(np.uint8)
        
        else:
            # Fallback to center overlay if no pose detected
            garment_resized = cv2.resize(garment_np, (w//3, h//2))
            y_offset = h//4
            x_offset = w//3
            
            gh, gw = garment_resized.shape[:2]
            if y_offset + gh <= h and x_offset + gw <= w:
                roi = result[y_offset:y_offset+gh, x_offset:x_offset+gw]
                blended = roi * 0.4 + garment_resized * 0.6
                result[y_offset:y_offset+gh, x_offset:x_offset+gw] = blended
        
        # Add AI processing indicator
        cv2.rectangle(result, (10, 10), (350, 60), (0, 0, 0), -1)
        cv2.putText(result, 'ðŸ¤– AI Virtual Try-On Active', (15, 35), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        cv2.putText(result, 'MediaPipe Pose Detection Used', (15, 55), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        return Image.fromarray(result)